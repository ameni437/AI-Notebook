{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ia8lTalyK2IV"
      },
      "outputs": [],
      "source": [
        "\"\"\"A simple world model\n",
        "\n",
        "Simple deterministic MDP is made of 6 grids (states)\n",
        "---------------------------------\n",
        "|         |          |          |\n",
        "|  Start  |          |  Goal    |\n",
        "|         |          |          |\n",
        "---------------------------------\n",
        "|         |          |          |\n",
        "|         |          |  Hole    |\n",
        "|         |          |          |\n",
        "---------------------------------\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from termcolor import colored\n",
        "\n",
        "\n",
        "class QWorld:\n",
        "    def __init__(self):\n",
        "        \"\"\"Simulated deterministic world made of 6 states.\n",
        "        \"\"\"\n",
        "        # 4 actions\n",
        "        # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
        "        self.col = 4\n",
        "\n",
        "        # 6 states\n",
        "        self.row = 6\n",
        "\n",
        "        # setup the environment\n",
        "        self.init_transition_table()\n",
        "        self.init_reward_table()\n",
        "\n",
        "        # reset the environment\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"start of episode\"\"\"\n",
        "        self.state = 0\n",
        "        self.count = 0\n",
        "        return self.state\n",
        "\n",
        "    def is_in_win_state(self):\n",
        "        \"\"\"agent wins when the goal is reached\"\"\"\n",
        "        return self.state == 2\n",
        "\n",
        "\n",
        "    def init_reward_table(self):\n",
        "     \"\"\"\n",
        "      0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
        "     ----------------\n",
        "     | 0 | 0 | 100  |\n",
        "     ----------------\n",
        "     | 0 | 0 | -100 |\n",
        "     ----------------\n",
        "     \"\"\"\n",
        "     self.reward_table = np.zeros([self.row, self.col])\n",
        "     self.reward_table[0, 2] = 100\n",
        "     self.reward_table[1, 2] = -100\n",
        "\n",
        "\n",
        "\n",
        "    def init_transition_table(self):\n",
        "     \"\"\"\n",
        "     actions:\n",
        "     0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
        "\n",
        "    states:\n",
        "    -------------\n",
        "    | 0 | 1 | 2 |\n",
        "    -------------\n",
        "    | 3 | 4 | 5 |\n",
        "    -------------\n",
        "     \"\"\"\n",
        "     self.transition_table = np.zeros([self.row, self.col], dtype=int)\n",
        "\n",
        "     self.transition_table[0, 0] = 0\n",
        "     self.transition_table[0, 1] = 3\n",
        "     self.transition_table[0, 2] = 1\n",
        "     self.transition_table[0, 3] = 0\n",
        "\n",
        "     self.transition_table[1, 0] = 1\n",
        "     self.transition_table[1, 1] = 4\n",
        "     self.transition_table[1, 2] = 2\n",
        "     self.transition_table[1, 3] = 1\n",
        "\n",
        "     self.transition_table[2, 0] = 2\n",
        "     self.transition_table[2, 1] = 5\n",
        "     self.transition_table[2, 2] = 2\n",
        "     self.transition_table[2, 3] = 2\n",
        "\n",
        "     self.transition_table[3, 0] = 3\n",
        "     self.transition_table[3, 1] = 3\n",
        "     self.transition_table[3, 2] = 4\n",
        "     self.transition_table[3, 3] = 0\n",
        "\n",
        "     self.transition_table[4, 0] = 4\n",
        "     self.transition_table[4, 1] = 4\n",
        "     self.transition_table[4, 2] = 5\n",
        "     self.transition_table[4, 3] = 1\n",
        "\n",
        "     self.transition_table[5, 0] = 5\n",
        "     self.transition_table[5, 1] = 5\n",
        "     self.transition_table[5, 2] = 5\n",
        "     self.transition_table[5, 3] = 2\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "     \"\"\"execute the action on the environment\n",
        "    Argument:\n",
        "        action (tensor): An action in Action space\n",
        "    Returns:\n",
        "        next_state (tensor): next env state\n",
        "        reward (float): reward received by the agent\n",
        "        done (Bool): whether the terminal state\n",
        "            is reached\n",
        "     \"\"\"\n",
        "     # determine the next_state given state and action\n",
        "     next_state = self.transition_table[self.state, action]\n",
        "     # done is True if next_state is Goal or Hole\n",
        "     done = (next_state == 2) or (next_state == 5)\n",
        "\n",
        "     # reward given the state and action\n",
        "     reward = self.reward_table[self.state, action]\n",
        "     # the environment is now in a new state\n",
        "     self.state = next_state\n",
        "     self.count += 1\n",
        "     return next_state, reward, done\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def print_cell(self, row=0):\n",
        "        \"\"\"UI to display agent moving on the grid\"\"\"\n",
        "        print(\"\")\n",
        "        for i in range(13):\n",
        "            j = i - 2\n",
        "            if j in [0, 4, 8]:\n",
        "                if j == 8:\n",
        "                    if self.state == 2 and row == 0:\n",
        "                        marker = \"\\033[4mG\\033[0m\"\n",
        "                    elif self.state == 5 and row == 1:\n",
        "                        marker = \"\\033[4mH\\033[0m\"\n",
        "                    else:\n",
        "                        marker = 'G' if row == 0 else 'H'\n",
        "                    color = self.state == 2 and row == 0\n",
        "                    color = color or (self.state == 5 and row == 1)\n",
        "                    color = 'red' if color else 'blue'\n",
        "                    print(colored(marker, color), end='')\n",
        "                elif self.state in [0, 1, 3, 4]:\n",
        "                    cell = [(0, 0, 0), (1, 0, 4), (3, 1, 0), (4, 1, 4)]\n",
        "                    marker = '_' if (self.state, row, j) in cell else ' '\n",
        "                    print(colored(marker, 'red'), end='')\n",
        "                else:\n",
        "                    print(' ', end='')\n",
        "            elif i % 4 == 0:\n",
        "                    print('|', end='')\n",
        "            else:\n",
        "                print(' ', end='')\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "    def print_world(self, action):\n",
        "        \"\"\"UI to display mode and action of agent\"\"\"\n",
        "        actions = { 0: \"(Left)\", 1: \"(Down)\", 2: \"(Right)\", 3: \"(Up)\" }\n",
        "        if self.count==0:\n",
        "          print(\"Start Game\")\n",
        "        else:\n",
        "          print(\"Action : \", actions[action])\n",
        "        for _ in range(13):\n",
        "            print('-', end='')\n",
        "        self.print_cell()\n",
        "        for _ in range(13):\n",
        "            print('-', end='')\n",
        "        self.print_cell(row=1)\n",
        "        for _ in range(13):\n",
        "            print('-', end='')\n",
        "        print(\"\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_episode(episode, delay=1):\n",
        "    \"\"\"UI to display episode count\n",
        "    Arguments:\n",
        "        episode (int): episode number\n",
        "        delay (int): sec delay\n",
        "\n",
        "    \"\"\"\n",
        "    os.system('clear')\n",
        "    for _ in range(13):\n",
        "        print('=', end='')\n",
        "    print(\"\")\n",
        "    print(\"Episode \", episode)\n",
        "    for _ in range(13):\n",
        "        print('=', end='')\n",
        "    print(\"\")\n",
        "    time.sleep(delay)\n",
        "\n",
        "def print_status(q_world, done,action, delay=1):\n",
        "    \"\"\"UI to display the world,\n",
        "        delay of 1 sec for ease of understanding\n",
        "    \"\"\"\n",
        "    os.system('clear')\n",
        "    q_world.print_world(action)\n",
        "    if done:\n",
        "        print(\"-------EPISODE DONE--------\")\n",
        "        delay *= 2\n",
        "    time.sleep(delay)"
      ],
      "metadata": {
        "id": "xM3FVC9cYlwl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate the environment\n",
        "q_world = QWorld()\n"
      ],
      "metadata": {
        "id": "tCYOCOPBK7YR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TODO:**\n",
        "\n",
        "\n",
        "A) Complete the code in the cells above (the parts to be completed are marked with 'TODO')\n",
        "\n",
        "B) Once part A) is complete, implement now each of the following situations in **a separate cell**. For the sake of illustration, Situation 1 is already implemented for you to give you a hint on how to answer each part. You could implement somthing similar for Situation 2, Situation 3 and Situation 4.\n",
        "\n",
        "**Situation 1:**take agent to goal in 2 steps. Print the episode name and display the grid for each step taken.\n",
        "\n",
        "**Situation 2:** take agent to H in 3 steps. Print the episode name and display the grid for each step taken.\n",
        "\n",
        "**Situation 3:** implement the following trajectory: down-right-up-right. what's the cumulative reward (assume the discount factor is 1)? Compare with the cumulative reward of the episode from **Situation 1** and comment.\n",
        "Make sure the cumulative reward is printed when the cell is executed.\n",
        "\n",
        "**Situation 4:** Implement an agent that takes random actions at each step and stops only when the task is solved (note that your agent may need to go through multiple episodes before your it is able to reach the goal). After how many episodes it solved the task? (the number of episodes should be displayed automatically each time you run the cell)"
      ],
      "metadata": {
        "id": "CwvXZezFg2js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Situation 1\n",
        "# print initial grid --before taking any action--\n",
        "state = q_world.reset()\n",
        "done = False\n",
        "episode = 1\n",
        "delay = 0\n",
        "print_episode(episode=episode, delay=0)\n",
        "\n",
        "# print initial status of the board\n",
        "print_status(q_world, done, 0, delay=delay)\n",
        "\n",
        "\n",
        "# to take the agent to GOAL (G) in two steps, the agent needs\n",
        "# to go right then go right again\n",
        "\n",
        "# recall the actions:\n",
        "# 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
        "\n",
        "# 1- Go right\n",
        "action=2\n",
        "next_state, reward, done = q_world.step(action)\n",
        "print_status(q_world, done,action, delay=delay)\n",
        "\n",
        "# 1- Go right again\n",
        "action=2\n",
        "next_state, reward, done = q_world.step(action)\n",
        "print_status(q_world, done,action, delay=delay)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUjj7jPFVSlU",
        "outputId": "77dbb5db-41aa-47a9-ddc9-d2b4072e3bde"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============\n",
            "Episode  1\n",
            "=============\n",
            "Start Game\n",
            "-------------\n",
            "| _ |   | G |\n",
            "-------------\n",
            "|   |   | H |\n",
            "-------------\n",
            "Action :  (Right)\n",
            "-------------\n",
            "|   | _ | G |\n",
            "-------------\n",
            "|   |   | H |\n",
            "-------------\n",
            "Action :  (Right)\n",
            "-------------\n",
            "|   |   | \u001b[4mG\u001b[0m |\n",
            "-------------\n",
            "|   |   | H |\n",
            "-------------\n",
            "-------EPISODE DONE--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Situation 2\n",
        "# TODO\n",
        "# Define the number of steps for Situation 2\n",
        "num_steps = 3\n",
        "\n",
        "# Print initial episode\n",
        "print_episode(1)\n",
        "\n",
        "# Run the environment for Situation 2\n",
        "for step in range(num_steps):\n",
        "    action = 1  # 1 corresponds to Down in the given code\n",
        "\n",
        "    # Execute the chosen action in the environment\n",
        "    next_state, reward, done = q_world.step(action)\n",
        "\n",
        "    print_status(q_world, done, action)\n",
        "\n",
        "    print_episode(step + 2)  # Episode count starts from 2 for Situation 2\n",
        "\n",
        "    if done:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "4JY-_Hm9MD6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d5153c-76b6-43bc-909d-ead0dc21fb72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============\n",
            "Episode  1\n",
            "=============\n",
            "Action :  (Down)\n",
            "-------------\n",
            "|   |   | G |\n",
            "-------------\n",
            "|   |   | \u001b[4mH\u001b[0m |\n",
            "-------------\n",
            "-------EPISODE DONE--------\n",
            "=============\n",
            "Episode  2\n",
            "=============\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Situation 3\n",
        "# TODO\n",
        "q_world = QWorld()\n",
        "\n",
        "# Situation 3 trajectory: down-right-up-right\n",
        "trajectory = [1, 2, 3, 2]\n",
        "\n",
        "# Run the environment for Situation 3 trajectory\n",
        "cumulative_reward_situation_3 = 0\n",
        "for action in trajectory:\n",
        "    next_state, reward, done = q_world.step(action)\n",
        "    cumulative_reward_situation_3 += reward\n",
        "    q_world.print_world(action)\n",
        "\n",
        "#  cumulative reward for Situation 3\n",
        "print(\"Cumulative Reward (Situation 3):\", cumulative_reward_situation_3)\n",
        "\n",
        "q_world.reset()\n",
        "\n",
        "# Situation 1 trajectory: down-right-up-right\n",
        "trajectory_situation_1 = [1, 2, 3, 2]\n",
        "\n",
        "#  the environment for Situation 1 trajectory\n",
        "cumulative_reward_situation_1 = 0\n",
        "for action in trajectory_situation_1:\n",
        "    next_state, reward, done = q_world.step(action)\n",
        "    cumulative_reward_situation_1 += reward\n",
        "\n",
        "#  cumulative reward for Situation 1\n",
        "print(\"Cumulative Reward (Situation 1):\", cumulative_reward_situation_1)\n"
      ],
      "metadata": {
        "id": "D7pnyph8iITn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea0752d-8920-416d-9e7e-d05b00c1da36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action :  (Down)\n",
            "-------------\n",
            "|   |   | G |\n",
            "-------------\n",
            "| _ |   | H |\n",
            "-------------\n",
            "Action :  (Right)\n",
            "-------------\n",
            "|   |   | G |\n",
            "-------------\n",
            "|   | _ | H |\n",
            "-------------\n",
            "Action :  (Up)\n",
            "-------------\n",
            "|   | _ | G |\n",
            "-------------\n",
            "|   |   | H |\n",
            "-------------\n",
            "Action :  (Right)\n",
            "-------------\n",
            "|   |   | \u001b[4mG\u001b[0m |\n",
            "-------------\n",
            "|   |   | H |\n",
            "-------------\n",
            "Cumulative Reward (Situation 3): -100.0\n",
            "Cumulative Reward (Situation 1): -100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Situation 4\n",
        "# TODO\n",
        "import random\n",
        "\n",
        "q_world = QWorld()\n",
        "\n",
        "#  a function to check if the task is solved\n",
        "def is_task_solved():\n",
        "    return q_world.is_in_win_state()\n",
        "\n",
        "#  a function to run the random agent until the task is solved\n",
        "def run_random_agent():\n",
        "    episode_count = 0\n",
        "    while not is_task_solved():\n",
        "        # Choose a random action (0: Left, 1: Down, 2: Right, 3: Up)\n",
        "        random_action = random.randint(0, 3)\n",
        "\n",
        "        # Execute the chosen random action in the environment\n",
        "        next_state, reward, done = q_world.step(random_action)\n",
        "\n",
        "        # Print the current status of the world\n",
        "        q_world.print_world(random_action)\n",
        "\n",
        "        # Check if the task is solved\n",
        "        if is_task_solved():\n",
        "            print(f\"Task solved in {episode_count + 1} episodes.\")\n",
        "            break\n",
        "\n",
        "        episode_count += 1\n",
        "\n",
        "    q_world.reset()\n",
        "\n",
        "run_random_agent()\n"
      ],
      "metadata": {
        "id": "jdApmbmYXWdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c80e75-d008-4439-9e1d-a80f7b273542"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action :  (Right)\n",
            "-------------\n",
            "|   | _ | G |\n",
            "-------------\n",
            "|   |   | H |\n",
            "-------------\n",
            "Action :  (Right)\n",
            "-------------\n",
            "|   |   | \u001b[4mG\u001b[0m |\n",
            "-------------\n",
            "|   |   | H |\n",
            "-------------\n",
            "Task solved in 2 episodes.\n"
          ]
        }
      ]
    }
  ]
}